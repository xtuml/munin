= HOWTO build and run the Protocol Verifier

Enough talk! <<Export MASL,Skip to the steps>>

If you want to run the latest version of the project without building,
<<Running the latest published version of the protocol verifier, see here>>

== Prerequisites

Before you get started, take a few moments to be sure you have the following
pieces in place.

=== Docker

If you plan to build and run the models, Docker is required for these processes.
Docker is a containerisation tool which can be used to produce an application
image that can be executed consistently on any platform which supports the
docker engine.

Two Docker terms you should be familiar with before you get started are "image"
and "container". An image is the blueprint for a container. It represents the
initial state of the filesystem when a container is created and carries metadata
about the image and the execution of the containerised application. You can
think of a container as an instance of an image. A container is created from an
image when the user requests it to run. Many containers can be created from the
same image simultaneously and are isolated from one another. Even after the
process inside the container exits, the container still exists and its state can
be interrogated until it is deleted by the user.

Follow the instructions here to install
link:https://docs.docker.com/get-docker/[Docker Desktop].

=== BridgePoint

Install the latest BridgePoint release. If you already have BridgePoint
installed, confirm that your version is more recent than 7.3.0
(2022-11-30) by checking the build ID.

Refer to
link:https://github.com/xtuml/bridgepoint/blob/master/doc-bridgepoint/process/HOWTO-install-bridgepoint.adoc[this guide]
for help installing BridgePoint and checking the build ID.

=== Git

BridgePoint comes packaged with everything you need to interact with the source
git repositories. That being said, some prefer to use a different GUI client
application or use the command line version of the tool.

Refer to
link:https://github.com/xtuml/bridgepoint/blob/master/doc-bridgepoint/process/HOWTO-use-git.adoc[the Git guide]
for a quick start on using git to clone repositories and import projects.

=== PlantUML

PlantUML is a tool for manipulating UML diagrams specified in a textual format,
which for the Munin project is constrained as the 'PLUS' domain-specific language.
PLUS is based on PlantUML Activity Diagrams. PlantUML can be downloaded from various
sources. The recommended approach is to load it as an Eclipse plugin so that it works
seamlessly with BridgePoint. The Eclipse plugin can be downloaded
link:https://plantuml.com/eclipse[here] and the update site is
link:http://hallvard.github.io/plantuml[here].

.**Getting PlantUML Working within BridgePoint**
[%collapsible]
====

To see PlantUML in BridgePoint, select the Window menu item, then select
Show View and then Other.  This will open a dialog showing a PlantUML
folder. Open the folder and select PlantUML.  Again using Window and Show
View, open a Project Explorer view. This will appear as a tab alongside the
Model Explorer and provides a view of the file structure. Sample PlantUML
files with the filename extension `.puml` can be found
link:https://github.com/xtuml/plus2json/tree/main/tests[here].

When first opening a `.puml` file right click on the file and select Open
With... then Text Editor.  Position the text editor pane and the PlantUML
graphic pane side by side. When the PlantUML text is edited the activity
diagram is updated automatically.

====

Whilst this is a generic PlantUML editor, it is used specifically for
editing the PLUS domain specific language. For details on PLUS see
link:https://github.com/xtuml/munin/blob/main/doc/howto/PLUS_guide.adoc[here].
For a tutorial on PLUS and explanation of supported Protocol Verifier
topologies, see
link:https://github.com/xtuml/plus2json/blob/main/doc/tutorial/AuditEventTopologyTutorial.pdf[here].

PlantUML and the PLUS language represent the "front end" of the Protocol
Verifier.  Together with the plus2json utility, this is how the Protocol
Verifier is configured and managed.

=== Source Projects

It is necessary to have the source code/models downloaded onto your machine. The
source code can be link:https://github.com/xtuml/munin[browsed on GitHub].
Follow the steps from the guide linked in the previous section to clone this
repository from GitHub. Follow the steps from the guide linked in the
<<BridgePoint>> section to import the following projects into a fresh
workspace:

* Protocol Verifier Domains
  ** AEReception (AER)
  ** AEOrdering (AEO)
  ** SequenceVerificationDataCentric (SVDC)
  ** InvariantStore (IStore)
  ** Verification Gateway (VG)
* Deployments
  ** AEO_SVDC (AEO + SVDC + IStore + VG)
  ** AER (a single domain deployment)
* Utilities
  ** utils
  ** plus2json

=== Terminology

==== Project

The term "project" typically refers to the model source and supporting
build/configuration files for a single unit of functionality. In our context,
this also maps one-to-one with an Eclipse/BridgePoint project.

==== Domain

A "domain" represents an encapsulated set of data objects and behaviours. The
term comes from the original Shlaer-Mellor method, and in MASL a domain is the
basic unit of compilation. Each domain exists in its own project.

==== Deployment

A "deployment" represents a combination of one or more domains including the
modelled connections between them. Deployments map to "project" in textual MASL
form, but the term "deployment" is preferred to avoid confusion with the more
general term "project".

==== Protocol Verifier

"Protocol Verifier" refers to the overall application as well as the BridgePoint
project where all of the domains are integrated together using a set of deployments.
In our context the Protocol Verifier is the collections of the two deployments:

* AER
* AEO_SVDC (includes iStore)

==== PLUS

"PLUS" is the name of the domain specific language for specifying valid job
sequences for the protocol verifier. PLUS is defined as a subset of constructs
from the PlantUML activity diagram syntax. A full description and guide for PLUS
can be found link:https://github.com/xtuml/munin/blob/main/doc/howto/PLUS_guide.adoc[here].

==== plus2json

"plus2json" refers to the application which processes PLUS files (textual
`.puml` activity diagrams) and produces JSON job definition and runtime
configuration files consumable by the Protocol Verifier (and the Audit
Event Simulator). plus2json can also be used to generate runtime test 
data based on the job definitions. It is capable of generating valid
runtime event streams and also of injecting errors into runtime event
streams.

plus2json source code, documentation and examples/tests can be found in a
git repository link:https://github.com/xtuml/plus2json[here].

== Overview

=== Toolchain

The Munin project team has been using the Shlaer-Mellor Method to model the
problem and our solution. The following sections describe the set of tools we
are using to actualise our designs.

==== xtUML/BridgePoint

The source models are represented graphically in xtUML using the BridgePoint
editor. Action semantics are encoded using MASL. The MASL specification also
defines all necessary "structural" constructs (e.g. classes, relationships,
state machines), but does not provide a
specification for capturing graphical layout. In addition, there is no existing
graphical tool which supports direct edit of MASL models. BridgePoint provides
the graphical editing experience required for structural Shlaer-Mellor modelling.

==== MASL

At build time, BridgePoint is used to export the xtUML models to textual MASL
format. As mentioned in the previous section, MASL is capable of representing
the complete semantics of the S-M method including structural elements. In
addition, MASL is required to be compatible with our selected backend code
generator (see next section).

==== MASL C++ code generator and software architecture (via Docker)

The code generator and target architecture chosen for Munin Phase 1 is the MASL
C++ software architecture published as Open Source Software in 2016. The code
generator consists of a Java-based MASL parser/generator which produces C++
source code for an application model. The code generator is designed to be
modular with a core translator and a set of peripheral translators that provide
additional capabilities such as Sqlite persistence, build file generation,
runtime model debugging, etc. The companion software architecture is a set of
runtime libraries written in C++ which provide mechanisms to implement the rules
of Shlaer-Mellor in a single threaded process. The code generator is designed
to produce generated code compatible with the runtime architecture.

.Click for more details
[%collapsible]
====

Since it was published in 2016, the MASL C++ project has been hosted
link:https://github.com/xtuml/masl[on GitHub] and managed by the xtUML
community. However, in the period since being published, the project has seen
little maintenance activity and has fallen behind the upstream version. As a
result, build tool and third party library dependencies have locked this
architecture in time. We have created a set of container images using Docker to
encapsulate the code generator and runtime libraries. This allows us to build
and execute models in two primary modes:

1. In the first mode, the project source MASL is passed into a container via a
shared folder where the code generator and C++ compiler are free to execute in
the context of all required dependencies. The generated C++ source code,
compiled libraries, executables, and supporting files are passed back out to the
development host machine using the same shared folder mechanism. Once compiled,
the binary file can be executed using another container image which provides all
necessary runtime shared libraries (MASL architecture and third party). This
mode of execution can be thought of as analogous to executing Java byte code in
an instance of the JVM.

2. In the second mode, an alternative Docker image is created by extending a base
image that already contains all of the MASL dependencies. The code is generated
and compiled during the image build phase and the resultant executable is set up
as the entrypoint for the image. Once the image is produced, it can be executed
on any platform that supports the docker engine.

The first mode is more flexible, better for iterative development, and results
in much smaller generated artefacts. The second mode is less error prone and is
more suited for deployment. We used the first mode primarily during the PoC and
have since switched to the second mode in anticipation of deployment and
scaling.

====

==== Testing and GitHub Actions (Continuous Integration)

We are using features of the MASL code generator along with custom domains to
define and run tests for each domain and the whole system. Like all other actions,
the tests are defined in MASL.  A domain service is created for each test.
Test are specially marked to be excluded from
the production application and are added to a test schedule. When the project is
built in the testing configuration, the tests are generated and executed and
results are logged to the console and output as a set of JSON files in the test
results directory.

We are using GitHub Actions to automatically build and run tests for each domain
and the system deployment any time new code is merged into the main development
branch of the repository. The output from each test schedule is consolidated and
formatted into an HTML report. The most recent report from the main branch can
be viewed link:https://s3.amazonaws.com/1f-outgoing/munin/main/index.html[here].
New failures or build issues are flagged before code is merged into the
mainline.

=== Domain overview

==== Protocol Verifier Domains

===== Reception (AER)

The role of the Audit Event Reception domain is to convert audit events received
from the monitored system, in whatever form they arrive, into audit event
objects that can be used by the other domains in the model. Any changes to the
format of received audit events will be addressed by the Audit Event Reception
domain. The format of delivery to the other domains will remain consistent. This
isolates the impact of changes in received audit event format to a single
domain. Unexpected input formats are rejected and errors are notified.

===== Ordering (AEO)

The role of the Audit Event Ordering domain is two-fold. It validates
audit event fields for reasonable and legal values, and it constructs
the audit event sequence into the correct order as determined by the previous
event ID in each Audit Event. Once the audit events have been correctly ordered
they are delivered to the Sequence Verification domain. Events from unexpected
sources are rejected and errors are notified. Audit Event Ordering waits for out
of sequence events to arrive for a defined period of time. Gaps in event
sequences not resolved within the defined time period are denoted as a failure
of the Job and the error condition is notified.

Another role of Audit Event Ordering is to read a configuration file at
initialisation, to use that data to set up its own definition classes, and to
forward that configuration information to Sequence Verification to set up its
definition classes. This approach ensures that the definition classes of Audit
Event Ordering and Sequence Verification are aligned.

===== Sequence Verification (SVDC)

The role of the Sequence Verification domain is to verify that the audit events
received are in a correct, expected order taking account of support for repeated
audit event types and forks, parallel branches and merges in the event
sequences. A Job is only deemed complete when all sequences within the Job have
completed. The Sequence Verification domain is built to detect and report a
number of error conditions in the received Audit Event data. These error
conditions include unexpected audit event types, unexpected sequences of audit
event types, sequences starting with the wrong audit event types and repetition
of audit event types in unexpected places.

===== Invariant Store (IStore)

The role of the Invariant Store domain is to provide persistent storage of
extra job invariants which will typically live longer than any single job.
The Invariant Store serialises access to persisted invariants across
multiple concurrent instances of Protocol Verifier processes.

For a visual overview of the domains and the key interactions between them click
link:images/MUNIN_Domains_and_Key_Interactions.pdf[here].

===== Verification Gateway (VG)

The role of the Verification Gateway is to generate audit events that represent the
protocol of job verification being preformed by the Protocol Verifier. These
audit events can then be injected into another (or the same) instance of the PV
and used to verify that the PV itself is behaving as expected. This means that the
PV is both observing and observable. This provides assurance that the PV is behaving
correctly and also serves to demonstrate the both the generation and consumption
of audit events. The PV can be said to be "eating its own dog food".

=== Deployment Overview

The Protocol Verifier is partitioned into a set of deployments that can be instantiated and
deployed in parallel to provide for performance scaling. The deployments are made up
of one or more of the Protocol Verifer domains. They are:

* Reception (AER) - mulitiple instances
* Ordering, Sequence Verification, iStore, Verification Gateway (AEO_SVDC) - multiple instances

For a description of how this architecture achieves scaling see
link:https://github.com/xtuml/munin/blob/main/doc/notes/MUN-151_scaling_ant.adoc[here].

=== Supporting Application Overview

==== plus2json

plus2json is an application that converts the PLUS language into JSON
files that the Protocol Verifier and the Audit Event
Simulator can consume. Since the Protocol Verifier is data driven, the use
of plus2json is essential to set up a new instance of the Protocol
Verifier to monitor and check a new protocol. Once configured and supplied
with a set of job definitions, the Protocol Verifier can be run without
reconfiguration for as long as the input set of job definitions need to be
monitored.

== Configuring the Protocol Verifier

This splits into two main parts:

* Specifying Job and Event Data Definitions
* Configuring Runtime Parameters

For a visual overview of the configuration of the Protocol Verifier (and the associated Audit Event Simulator) 
click link:images/MUNIN_Configuration_Setup.pdf[here].

=== Specifying Job and Event Data Definitions

Each protocol monitored by the Protocol Verifier needs a __Job
Definition__ which specifies the behaviour (protocol) of a particular
monitored task.  Job definitions are defined in the PLUS language and
edited/visualised with PlantUML.

The Protocol Verifier is data driven. It has no built-in knowledge of any
particular protocol. Prior to running the Protocol Verifier for the first
time, a runtime configuration file and job definition files need to be in
place for the protocols being monitored. After this has been done
initially, it needs to be repeated only to add or change the definitions of
monitored jobs.  The configuration for the protocol verifier is found in
the `deploy` directory. This is loaded at start up and checked on a
regular basis for updates.

The structure of the `deploy` directory is shown link:images/MUNIN_Domains_and_Key_Interactions.pdf[here].

==== PLUS Job Definitions

Use PLUS to define jobs and produce runtime configuration.

Refer to
link:https://github.com/xtuml/munin/blob/main/doc/howto/PLUS_guide.adoc[here]
for details on the use of PLUS.

==== plus2json: PLUS Conversion and configuring the Protocol Verifier

Use plus2json to convert PLUS into job definitions and runtime
configuration files.

The plus2json application takes as input PLUS (`.puml`) text files and
produces a number of possible outputs.  The primary output is the
JSON-formatted job definition for a particular protocol.  plus2json also
can produce runtime configuration files in a format that the Protocol
Verifier and the Audit Event Simulator can consume.

For details on using plus2json and its options refer to
link:https://github.com/xtuml/plus2json[plus2json].

The commands described below produce a single config.json file which contains a 
number of configuation parameters and a list of the job specification
names together with individual job configuation parameters. In addition they 
generate a file for each job definition and the event definitions relevant to 
that job definition. Further, if the job definition requires any supplementary 
audit event data then an additional configuration file defining the audit event 
data definitions for that job definition can be created.

Currently, PLUS files should each contain a single job defintion.

Assume a PlantUML file containing a single job definition called `Tutorial_1.puml` 
has been created. The typical sequence of plus2json commands is as follows:

Optionally backup the existing configuration file by moving the contents of 
link:https://github.com/xtuml/munin/deploy/plus2json-deployed[this directory] 
to a backup location of your choice.

The following commands convert the PLUS files into json files and load them into the appropriate
configuration directories: 

. `python plus2json.pyz Tutorial_1.puml` 
- this checks the syntax of the puml file
. `python plus2json.pyz Tutorial_1.puml --job -p` 
- this produces a human readable representation of the job definition including previous events and audit event data
. `python plus2json.pyz Tutorial_1.puml --job | python -m json.tool > munin/deploy/plus2json-deployed/generated-config/tutorial_1.json` 
- this generates the job definition file and loads it into the appropriate directory. The job definition file 
must be the same name as the job definition name 
. `python plus2json.pyz Tutorial_1.puml --audit_event_data |  python -m json.tool > munin/deploy/plus2json-deployed/generated-config/tutorial_1_aedata.json` 
- this generates the audit event data associated with the job definition and loads it into the appropriate directory. 
Currently, the file must be manually edited to create a file per audit event data item. Each file must have the same name
as the audit event data item. For example, if the tutorial_1_aedata.json file contains an Invariant X and an Invariant Y
then a file X.json and a file Y.jon need to be created containing the respective invariants.
. `python plus2json.pyz Tutorial_1.puml --aeo_config |  python -m json.tool > munin/deploy/plus2json-deployed/generated-config/config.json` 
- this loads the master config.json file which sets a number of behavioural parameters and references each of the jobs

Note: Advanced feature - the behavioural parameters in the config.json file can be adjusted with appropriate caution

.Click to see more details on the `deploy/plus2json-deployed/generated-config/config.json` config file format
[%collapsible]
====

These configuration files contain some items that may be adjusted to manage the
digital twin. The following is a list of the configuration items that can be
adjusted and there description are as follows:

*SpecUpdateRate* - A time period that determines how often the application
reloads the configuration files.

*IncomingDirectory* - The directory where the application expects to find JSON
files containing events.

*ProcessingDirectory* - The directory where the application moves JSON files
while the events are being processed.

*ProcessedDirectory* - The directory where the application moves JSON files
after all the contained events are processed.

*EventThrottleRate* - The rate that events should be processed from one
reception file specified as a duration. Once an event has been processed a timer
shall be set to the throttle rate and the timer shall have to expire before
another event is selected for processing.

*ReceptionDeletionTime* - When a file has been through reception the details of
the reception processing shall be stored until this time expires.

*ConcurrentReceptionLimit* - A number that indicates the limit of concurrent
reception jobs that can be executing, e.g. 1 = one active reception job

*MaxOutOfSequenceEvents* - This is the consecutive maximum out of sequence
events that can be received for a job before an error is declared.

*MaximumJobTime* - This is the maximum time it should take for a job to be
finished. When this time has been reached after the job was started it shall be
archived if there are no blocked events or failed if there are blocked events.

*JobCompletePeriod* - When a Job has completed it shall be either archived or
failed and once the job complete period has expired it shall be deleted from the
domain with all associated events.

*JobStoreLocation* - This contains the relative path to the directory where the 
Job Store Id file will be created and maintained.

*JobStoreAgeLimit* - This defines how long the job ids will be retained in the job
store.

*InvariantStoreLoadRate* - This defines how frequently the invariant store is 
checked for changes. The detection of changes will prompt the upload of the new 
invariants to each running instance of the AEO_SVDC process.

For each *Job Definition* the following are defined:

*JobDefinitionName* - this is provided automatically by the plus2json application
(This is also known as JobTypeName)

*JobDeprecated* - this is a boolean which indicates whether the job definition is in 
current use. `false` means the job definition is in current use.

*JobTypeExpiryDate* - this is a timestamp that sets a date and time for when the job 
definition is no longer valid.

*StaleAuditEventDuration* - this is the period of time for which a received event is
deemed to be valid after its stated auditEventTime. If the event is received 
outside of this valid period then the audit event is considered to have failed.

*BlockedAuditEventDuration* - this is the period of time for which a job will wait
for an expected event (as indicated by its previous event id on earlier event) to arrive.
If the expected event fails to arrive within this period then the job will be failed.

Example:

----
{
    "SpecUpdateRate": "PT2M",
    "MaxOutOfSequenceEvents": 10,
    "MaximumJobTime": "PT10M",
    "JobCompletePeriod": "PT24H",
    "IncomingDirectory": "incoming",
    "ProcessingDirectory": "processing",
    "ProcessedDirectory": "processed",
    "EventThrottleRate": "PT0S",
    "ReceptionDeletionTime": "PT10M",
    "ConcurrentReceptionLimit": 1,
    "JobStoreLocation": "./JobIdStore",
    "JobStoreAgeLimit": "P7D",
    "InvariantStoreLoadRate": "PT2M",
    "Jobs": [
        {
            "JobDefinitionName": "Job with Simple Sequence",
            "JobDeprecated": false,
            "JobTypeExpiryDate": "2022-04-11T18:08:00Z",
            "StaleAuditEventDuration": "P99W",
            "BlockedAuditEventDuration": "PT5M"
        },
        {
            "JobDefinitionName": "AND Fork",
            "JobDeprecated": false,
            "JobTypeExpiryDate": "2022-04-11T18:08:00Z",
            "StaleAuditEventDuration": "P99W",
            "BlockedAuditEventDuration": "PT5M"
        },
        {
            "JobDefinitionName": "XOR Fork",
            "JobDeprecated": false,
            "JobTypeExpiryDate": "2022-04-11T18:08:00Z",
            "StaleAuditEventDuration": "P99W",
            "BlockedAuditEventDuration": "PT5M"
        },
        {
            "JobDefinitionName": "AND Fork and Merge",
            "JobDeprecated": false,
            "JobTypeExpiryDate": "2022-04-11T18:08:00Z",
            "StaleAuditEventDuration": "P99W",
            "BlockedAuditEventDuration": "PT5M"
        },
        {
            "JobDefinitionName": "Job with Intra-Job Invariant",
            "JobDeprecated": false,
            "JobTypeExpiryDate": "2022-04-11T18:08:00Z",
            "StaleAuditEventDuration": "P99W",
            "BlockedAuditEventDuration": "PT5M"
        }
    ]
}
----

====
Additionally if the simulator is to be used also execute these:

. `python plus2json.pyz Tutorial_1.puml --play --aesim_config | python -m json.tool > munin/deploy/aesim-config/plus2json/Tutorial_1.json` 
- this creates a job definition file for Tutorial_1 for consumption by the AE Simulator. The file name of this file must match
the job type name.
. `python plus2json.pyz Tutorial_1.puml --play --aesim_test | python -m json.tool > munin/deploy/aesim-config/plus2json/plus2json-test-specification.json` 
- this creates an example dispatch file for Tutorial_1 for consumption by the AE Simulator

==== Configuring the simulator

The Audit Event Simulator will use the configuration files that are placed in the deploy/aesim-config/ directory.
There can be multiple configurations files in this folder.

It is possible to configure the simulator to generate jobs that are to be
delivered to the digital twin. The configuration file it loads is defined in the
command specified in the `ae-simulator.yml` and passed on the
`-test-config` argument as shown here:

  command: "-test-config config/plus2json/plus2json-test-specification.json -postinit schedule/startup.sch -util Inspector -inspector-port 10 -util MetaData"

It ispossible to edit the yml file to point atthe configuration file that the simulator should load.

.Click to see more details on the config file format
[%collapsible]
====

The JSON test specification configuration file allows the user to define the test to be executed.
Below is a sample of a test from the configuration file:

  {
  "OneFilePerJob" : "false",
  "MaxEventsPerFile" : 170,
  "FileTimoutPeriod" : "PT10S",
  "JobSpecificationLocation" : "config/plus2json",
  "TestFileLocation" : "test-files/generated",
  "TestFileDestination" : "test-files/incoming",
  "Tests" : [{
      "TestId" : 1,
      "TestName" : "Ordered_Test1",
      "TotalTests" : 100,
      "TestFrequency" : "PT0.15S",
      "TestJobSpec" : [{
            "TestJobSpecName" : "Job with Simple Sequence",
            "EventDispatchOrder" : "1,2,3,4,5"
        },
        {
            "TestJobSpecName" : "AND Fork",
            "EventDispatchOrder" : "1,2,3,4,5,6,7,8,9"
          },
          {
            "TestJobSpecName" : "XOR Fork",
            "EventDispatchOrder" : "1,2,3,4,5,6,7,8"
          },
          {
            "TestJobSpecName" : "AND Fork and Merge",
            "EventDispatchOrder" : "1,2,3,4,5,6,7,8,9"
          },
          {
            "TestJobSpecName" : "Job with Intra-Job Invariant",
            "EventDispatchOrder" : "1,2,3,4,5"
          }]
      }]
}



The following provides an explanation to each of the JSON elements in the configuration
file:

*OneFilePerJob* - If set true then all events for a job are put into one event file for an execution of a job. Once all events have been added the event file is made available. If set to false events from any in progress Job will be put in the event file and the simulator will use the MaxEventsPerFile and FileTimeoutPeriod to determine when the event file should be made available.

*MaxEventsPerFile* - When the OneFilePerJob is set to false this is the maximum number of events that shall be added to an event file.

*FileTimoutPeriod* - When the OneFilePerJob is set to false this is the maximum amount of time that shall pass before the event flies made available.

*JobSpecificationLocation* - The details of the events that are to be generated for a job are captured in a Job Specification. This value identifies the location of these files.

*TestFileLocation*- This is the directory where the files should be created.

*TestFileDestination* - This directory the files should be moved to once
created.

*Tests* - This is an array of tests that are to be executed by the simulator. The following provides and explanation of the JSON elements in a test.

*TestId* - This provides a unique id for the test specification.

*TestName* - The name that has been assigned to the test.

*TotalTests* - This defines the total number of times this test is to be
executed.

*TestFrequency* - A duration that specifies the frequency that the test should
be run e.g. PT1S is every second.

*TestJobSpec* - This is an array of the test job specifications that should be executed for this test. The following provides an explanation of the JSON elements in a test job specification.

*TestJobSpecName* - This the name of the JSON file for the test job specification e.g., EndToEndHappyPathJob.json that should be found in *JobSpecificationLocation*. A test job specification indicates the events that are to be produced when the job is executed for the test. Note that this is may not be a valid set of events for a given job that is to be validated by the Protocol Processor but is simulating a set of events in a scenario that needs to be tested

*EventDispatchOrder* - Each event specified in the job specification has a unique identifier. This element allows the tester to specify the order that the events are to be dispatched for this test.

The JSON job specification configuration file allows the user to define the events that are in a job run that is to be executed by the simulator.
Below is a sample of a test from the configuration file:


  {
    "JobSpecName": "scenario1",
    "JobName": "Job with Simple Sequence",
    "EventDefinition": [
        {
            "EventId": "1",
            "PreviousEventId": "None",
            "EventName": "A1",
            "SequenceStart": "true",
            "NodeName": "default_node_name",
            "ApplicationName": "default_application_name"
        },
        {
            "EventId": "2",
            "PreviousEventId": "1",
            "EventName": "B1",
            "SequenceStart": "false",
            "NodeName": "default_node_name",
            "ApplicationName": "default_application_name"
        },
        {
            "EventId": "3",
            "PreviousEventId": "2",
            "EventName": "C1",
            "SequenceStart": "false",
            "NodeName": "default_node_name",
            "ApplicationName": "default_application_name"
        },
        {
            "EventId": "4",
            "PreviousEventId": "3",
            "EventName": "D1",
            "SequenceStart": "false",
            "NodeName": "default_node_name",
            "ApplicationName": "default_application_name"
        },
        {
            "EventId": "5",
            "PreviousEventId": "4",
            "EventName": "E1",
            "SequenceStart": "false",
            "NodeName": "default_node_name",
            "ApplicationName": "default_application_name"
        }
    ]
}


The following provides an explanation to each of the JSON elements in the test job specification configuration file. It identifies som of the data that shall by the simulator for each event e.g., EventName, PreviousEventId, etc:

*JobSpecName* - The name of the test job specification that shall be executed in the test.

*EventDefinition* - An array of events that will be generated for this job. The following provides an explanation of the JSON elements in an event.

*EventId* - A unique identifier for this event specification in the test job specification.

*EventName* - The name of the event that is to be generated and presented to the protocol processor.

*NodeName* - The name of the node that is responsible for generating the event.

*ApplicationName* the name of the application that generated the event.

*DispatchDelay* - The amount of time that the simulator should wait before dispatching the event on a test run for this job specification.

*SequenceStart* - Indicates if this is the first event in a sequence of events. If so there will not be a previous event definition id.

*PreviousEventId* - This is the previous event definition id e.g. in this event definition with an id of 2 the previous event definition is 1.

*EventData* - An array of event data that should be generated by the simulator for the event. The following provides an explanation of the JSON elements in an event data.

*DataName* - The name of the event data e.g., PersistentInvariant.

*DataValue* - The value of the data that is to be supplied for the event data e.g. "PersistentInvariant" : "someDataValue".

====

=== Configuring the Deployment

This configuration determines the number of instances of each of the deployments to instantiate at start up.
Note: This version of the Protocol Verifier supports static scaling.

Two different methods have been used for scaling the containers for the protocol verifier. 
The first used be AER makes use of the

    `--scale aereception=2` 

command line option provided by docker. This instructs the docker engine how many instances of the service named `aereception` 
to create. This method is being used by AER as we do not to configure each running instance with specific configuration data.
AER is responsible for arranging events by JobId and each instane of aereception shall process the incoming event files and 
place events for each job identified in a file in the `verifier-incoming` directory. The files placed in `verifier-incoming` 
are arranged into directories fomr 00 to ff and AER places the file for the job into the directory that matches the last two 
digits in the job id.

    `jobId = cadf665f-fc78-4d84-9673-0ab2eb2c8b4d` 

would be placed in

    `verifier-incoming/4d` 

In order to ensure that an individual job is only every processed by one instance of AEO_SVDC this is provided with configuration 
data in its docker compose file that indicate which job ids it is to process and we refer to this as its `Job Group`. An example can
seen in the following compose file:
[source,yml]
----
version: "3.9"
services:
  aeo_svdc-1:
    image: "ghcr.io/xtuml/aeo_svdc"
    build:
      context: .
      args:
        EXTRA_MASL_ARGS: "-skiptranslator Sqlite"
    ports:
      - "20000:20000"
      - "30000:30000"
      - "40000:40000"
    volumes:
      - "./plus2json-deployed/generated-config:/root/config"
      - "./InvariantStore:/root/InvariantStore"
      - "./verifier-incoming:/root/incoming"
      - "./JobIdStore:/root/JobIdStore"
      - "./verifier-processed:/root/processed"
    command: "-configPath config/  -preinit schedule/startup.sch -startJobGroup 00 -endJobGroup FF"
----

In the above example we have defined one service `aeo_svdc-1` that has a `startJobgroup` of `00` and
an `endJobGroup` of `FF`. This means that it will process all audit event files it finds in the directories
in its `verifier-incoming` directory from `00` to `FF` e.g., an eventfile for the job

    `jobId = cadf665f-fc78-4d84-9673-0ab2eb2c8b4d` 

placed in

    `verifier-incoming/4d` 

would be processed by the instance `aeo_svdc-1`

As we have to inform the AEO_SVDC instance of its `Job Group` in order to scale this processing we have to extend the docker compose file. The following is an example of AE_SVDC being scaled to two instance.

[source,yml]
----
version: "3.9"
services:
  aeo_svdc-1:
    image: "ghcr.io/xtuml/aeo_svdc"
    build:
      context: .
      args:
        EXTRA_MASL_ARGS: "-skiptranslator Sqlite"
    ports:
      - "20000:20000"
      - "30000:30000"
      - "40000:40000"
    volumes:
      - "./plus2json-deployed/generated-config:/root/config"
      - "./InvariantStore:/root/InvariantStore"
      - "./verifier-incoming:/root/incoming"
      - "./JobIdStore:/root/JobIdStore"
      - "./verifier-processed:/root/processed"
    command: "-configPath config/  -preinit schedule/startup.sch -startJobGroup 00 -endJobGroup 7F"
  aeo_svdc-2:
    image: "ghcr.io/xtuml/aeo_svdc"
    build:
      context: .
      args:
        EXTRA_MASL_ARGS: "-skiptranslator Sqlite"
    ports:
      - "20001:20001"
      - "30001:30001"
      - "40001:40001"
    volumes:
      - "./plus2json-deployed/generated-config:/root/config"
      - "./InvariantStore:/root/InvariantStore"
      - "./verifier-incoming:/root/incoming"
      - "./JobIdStore:/root/JobIdStore"
      - "./verifier-processed:/root/processed"
    command: "-configPath config/  -preinit schedule/startup.sch -startJobGroup 80 -endJobGroup FF"
----

In this example `aeo_svdc-1` has been assigned `00` to `7F` and 'aeo_svdc-2' has been assigned `80` to `FF`.

NOTE: Care should be taken when assigning `Job Group` as there is no verification that the instanes have not been assigned overlapping groups.

== Building and Running the Protocol Verifier

=== Build Overview

As mentioned in the section discussing the toolchain, there are three major
steps to building and running the projects:

. Export MASL
. Build with Docker
. Launch with Docker

Before getting into the actual build, it is often an instructive process to
go through the project structure file by file and explore the purpose of each
file in the context of the build. We will use the `AEReception` domain for this.
Each of the other domains follows a similar pattern. Not every file/directory
seen here will exist in each domain project.

For a visual overview of the top-level of the project directory structure click  
link:images/MUNIN_Top_Level_Project_Directory_Structure.pdf[here].

NOTE: Some files are marked by git as "ignored" these tend to be generated
byproducts of the build that should not be committed to the repository (e.g.
build logs, test results). Not every one of these files will be covered in the
section below, but it is good to be aware of them.

  ▾ AEReception/
    ▸ config/
    ▾ gen/
      ▸ code_generation/
        application.mark
        features.mark
        README.adoc
    ▸ masl/
    ▸ models/
    ▸ schedule/
    ▸ schema/
    ▸ test_results/
    ▸ testing/
      CMakeLists.txt
      docker-compose.test.yml
      docker-compose.yml
      Dockerfile

For a visual overview of the directory structure for the AEReception domain 
click link:images/AEReception_Directory_Structure.pdf[here].

==== `config`

The `config` directory contains plaintext files used by the application itself
to configure the domain. The application is passed a config file as a command
line argument, which it parses and uses to set up the initial instance
population. Not all projects have config folders.

==== `gen/`

The `gen` directory contains files used during the process of code generation
and build. The `AEJSON_OOA/` subdirectory contains a handwritten C++ utility
domain used by the main `AEReception` domain to parse the JSON input files.
`features.mark` and `application.mark` contain model compiler "marks". These
metadata are associated with particular application model elements and act as directives
to the compiler. For example, domain services used exclusively for testing are
marked as `test_only`, and the architecture will exclude them from generation
during a production build.

==== `masl/`

The `masl` directory is the output location for exported MASL text. When the
project is clean, this directory is empty. The files in this directory are
generated and should not be hand edited.

==== `models/`

The `models` directory is where BridgePoint stores xtUML source model files. The
files in this directory are managed by BridgePoint and should not be hand
edited.

==== `schedule/`

The `schedule` directory contains plaintext files used by the architecture for
startup and testing. The MASL C++ platform provides a mechanism to run domain
services externally using a schedule file. This mechanism is particularly useful
for setting up execution of a particular set of tests, however it can also be
leveraged to determine which services will run at different stages of
initialisation.

TODO Should we mention SKIP and PAUSE commands here?

==== `schema/`

The `schema` directory holds the json schemas relevant to the domain. Not all
domains will have a schema directory

==== `test_results/`

The `test_results` directory is created during a test execution and contains
JSON files containing the results and details of executed tests. This directory
is created by the execution of the unit tests. The files should not be hand
edited and this directory may not exist before a run.

==== `testing/`

The `testing` directory contains test files used in the unit tests.

==== `CMakeLists.txt`

The `CMakeLists.txt` file is the top-level build file for the C++ build after
code generation is complete. It sets up link paths, include directories and
other global properties for the build. The behaviour of the build is different
depending on whether or not the build is launched in `Debug` (test) mode.

==== `docker-compose.yml` and `docker-compose.test.yml`

The two `docker-compose*.yml` files are used to configure the build and
execution using Docker. The build target and parameters are configured, internet
ports and shared volumes are set up between host and container, and the command
line arguments are defined for the process. Any `docker compose ...` commands
will reference `docker-compose.yml` by default, however the config file can be
altered by passing the name of the new file with `-f`. You will notice in the
rest of this document that any time the test build is being run, the command
will start with `docker compose -f docker-compose.test.yml ...`.

==== `Dockerfile`

The `Dockerfile` file defines the actual build. The file uses Docker's
multi-stage build capability which allows the file to define two separate builds
-- one for testing and one for release. The testing version of the image
executes the code generator in test mode which will cause it to generate code
for the `test_only` services as well as additional scaffolding for testing
interfaces.

TIP: I encourage you to take a moment before moving on to open each of these
files in a text editor (you can double click them from the "Project Explorer"
within BridgePoint) and briefly explore their contents.

For completeness a visual overview of the top-level of the deployment directory structure is
available by clicking link:images/MUNIN_Top_Level_Deployment_Directory_Structure.pdf[here], and
a more detailed example for the AEO_SVDC deployment is illustrated by clicking 
link:images/AEO_SVDC_Directory_Structure.pdf[here]

=== Export MASL

. Open up BridgePoint. Assure that you have all of the source projects
imported into your workspace.

+
See the <<Source Projects,list of projects>>.

. To export MASL, select each project and click the
link:images/01_hammer.png[hammer icon] found in the tool ribbon at the top of
the screen.
. Alternatively you can right click each project and select
link:images/02_build_project.png["Build Project"] from the context menu.
. If you wish to export MASL for all projects at once, you can click
link:images/03_build_all.png["Build All"] from the "Project" menu in the
application bar at the top of the application or use the `Ctrl-B`/`Cmd-B`
keyboard shortcut.

NOTE: The `utils` project simply contains common MASL interfaces and need not be
built. In fact, it will not even show up in the xtUML Modelling perspective.

=== Build each domain

This should be repeated for each of the domains and the simulator domain.
For the sake of demonstration, the following instructions will
reference the `AEReception` domain. Additionally, the instructions will assume
that the code has been cloned in a standard location (`~/git/munin` on Linux/Mac
and `C:\git\munin` on Windows).

. Open a shell and navigate to the `AEReception` project directory:

  cd ~/git/munin/models/AEReception   # linux/mac
  cd C:\git\munin\models\AEReception  # windows

. Build the project with Docker by running the following command:

  docker compose build --no-cache

+
NOTE: The `--no-cache` flag tells Docker to rebuild the image even if it has
been built before and exists in cache. This is not strictly necessary, but it
provides confidence that the latest code is being used for the build.

+
NOTE: The first invocation of `docker compose build ...` will cause Docker to
download the base MASL image from Docker Hub. This image is > 600 MB and
therefore will take some time to download. Subsequent builds will not require
this download.

A graphical rendition of the domain build process is depicted
link:images/Building_a_Domain.pdf[here].

. Repeat this step for the other domains:
  .. AEOrdering
  .. AESimulator
  .. SequenceVerificationDataCentric
  .. InvariantStore

=== Build each Deployment

IMPORTANT: Before this step, all of the application domains must be built
as demonstrated in the previous step.

The following deployments need to be built

* AEO_SVDC containing Ordering, Sequence Verification and the InvariantStore domains

TODO Check the following describes the deployment build process

. In your shell, navigate to the AEO_SVDC directory:

  cd ~/git/munin/models/AEO_SVDC   # linux/mac
  cd C:\git\munin\models\AEO_SVDC  # windows

. Build the project with Docker by running the following command:

  docker compose build --no-cache
  
A graphical rendition of the deployment build process is depicted
link:images/Building_a_Deployment.pdf[here].

=== Running the protocol verifier

==== The `munin/deploy` Folder

.It is useful but not essential to have a good understanding of the structure
of the deploy folder. Click to see more details on the deploy folder
[%collapsible]
==== 

This section describes the purpose of the folders and files within the 
`munin/deploy` folder.


  ▾ deploy/
    ▸ aesim-config/
      ▸ plus2json/
    ▸ reception-config/
    ▾ plus2json-deployed/
      ▸ generated-config/
    ▸ aer-incoming/
    ▸ verifier-incoming/
    ▸ verifier-processed/
    ▸ InvariantStore/
    ▸ JobIdStore/
    ▸ puml/
      ae-simulator.yml
      reception.yml
      verifier.yml
      README.adoc

`aesim-config/plus2json/`

This contains files that capture the test specification and job specifications that are too be used by the Audit Event Simulator. Typically these will have been generated using plus2json. 

`reception-config/`

This contains the configuration file for the Audit Event Reception (AER). 

`plus2json/generated-config/`

This contains files that capture the test specification and job specifications that are too be used by the AEO_SVDC. Typically these will have been generated using plus2json. 

`aer-incoming/`

This is the directory that AER shall check for incoming Audit Event files. The Audit Event Simulator shall place Audit Event files that it generates in this directory. 

`verifier-incoming/`

This is the directory that AEO_SVDC shall check for incoming Audit Event files. AER shall place Audit Event files that it has processed in this directory. 

`verifier-processed/`

This is the directory that AEO_SVDC shall place Audit Event files that it has processed in this directory. 

`puml/`

This is the directory contains example PLUS files. 

`ae-simulator.yml`

This is the docker compse file that is used by the Audit Event Simulator. 

`reception.yml`

This is the docker compse file that is used by the Audit Event Reception. 

`verifier.yml`

This is the docker compse file that is used by AEO_SVDC. 

`README.adoc`

An explanition of the directory. 

====

==== Executing the Processes

TODO Check the following:
This section looks ok to me but in future we should use the -d option 
and have the containers running in the background. When we do this we need to makesure that the
log files are not getting sent to the console bit to a log file that is accessible outside the
container and is managed using rotate. Pretty sure the logging domain will do this for us. Greg.

IMPORTANT: Before this step, all of the domains and deployments 
described above must be built as demonstrated in the previous steps.

The Protocol Verifier is run as two separate processes. The first contains
the single domain AEReception. Having been built it can be run using the 
command below. The second is the AEO_SVDC deployment which contains the 
three domains AEOrdering, SequenceVerification and InvariantStore. Again,
having been built it can be run using the command below.

In addition, a third process can be launched containing the AESimulator
domain. This is required if a simulated source of audit events is required.
This is covered <<Running the protocol verifier with the audit event simulator,here>>.

. Open a shell in the `munin/deploy` directory, run the following command
to launch the AEO_SVDC process:

  docker compose -f verifier.yml up

. Open another shell in the `munin/deploy` directory, run the following command
to launch the AEReception process:

  docker compose -f reception.yml up

+
You should see some logs begin to appear, in particular a periodic log from the
`AEReception` domain that it is waiting on input files.

When running without the simulator it is possible to use previously processed files.
However if doing this it is important that the files in the JobIdStore are removed prior
to running the software. Failure to do so will result in the following error:

  `Duplicate Job Id identified`

. Open a file explorer and navigate to the `deploy/verifier-processed/`
directory within the repository. This can be done through your OS GUI or quickly
through a simple command:

  open -a Finder ~/git/munin/deploy/verifier-processed/  # mac
  xdg-open ~/git/munin/deploy/verifier-processed/        # linux
  explorer C:\git\munin\deploy\verifier-processed\       # windows

. Open another file explorer window and navigate to the
`deploy/aer-incoming/` directory within the repository:

  open -a Finder ~/git/munin/deploy/aer-incoming/  # mac
  xdg-open ~/git/munin/deploy/aer-incoming/        # linux
  explorer C:\git\munin\deploy\aer-incoming\       # windows

. link:images/04_drag_and_drop.gif[Drag and drop] e.g, `871f08bc-324e-4a8c-8d5d-bd052d3ba0ed` from the
`verifier-processed/` directory to the `aer-incoming/` directory. You will see a flurry of
output from the application and the file will reappear in the `verifier-processed/`
directory. Inspect the logs and you will see that the audit events have been
received, ordered, and verified by the application.

. Kill the process by pressing `Ctrl-C`. Clean up the process by running the
following command:

  docker compose down

==== Troubleshooting Docker

Docker is a great tool for standardising builds and deployments, however it
presents some pitfalls when being used as a local build/development tool.

Docker Compose requires the "down" command to be issued even after all the
processes launched by the "up" command have terminated. This is because though
the process inside each container has exited, the container itself still exists
and can be restarted. As long as the container exists (whether running or
stopped), it will hold onto resources such as shared volumes and internet ports.
The "down" command tells Docker Compose to remove all the containers associated
with the launch.

If you see the message "port is already allocated", it is likely that you forgot
to run the `docker compose down` command somewhere along the way. When you run
this command, make sure it matches the "up" command (e.g. if you run `docker
compose -f docker-compose.test.yml up` to start the application, you should run
`docker compose -f docker-compose.test.yml down` in the same directory to tear
it down.)

If there is only one command to remember from this section, it is this:

  docker system prune

This command causes Docker to remove all stopped containers, networks, dangling
images and build cache. This usually works to give a "fresh" start if you get
stuck.

If you are making changes but not observing different behavior check the
following:

. Assure you have re-exported MASL (build projects from within BridgePoint)
. Run the build again with caching disabled: `docker compose build --no-cache`
. Run the "up" command with the `--force-recreate` flag: `docker compose up
--force-recreate` (this flag forces existing containers to be replaced with new
ones created from the latest image).

==== Running the latest published version of the protocol verifier

As mentioned above, the application is built and published automatically each
time new code is merged into the main repository branch. It is possible to use
docker to run the latest version of the application without any build at all.

. Authenticate with the GitHub Container Registry by executing the command:

  docker login ghcr.io

+
Use your GitHub.com account name and password to log in. If you have two-factor
authentication enabled on your account, you will have to create a new personal
access token to use in place of your password. Follow the guide
link:https://github.com/xtuml/bridgepoint/blob/master/doc-bridgepoint/process/HOWTO-use-git.adoc#generating-authentication-credentials[here]
to generate a new token with the
link:images/08_read_packages.png["read:packages"] scope.

. Execute the application by running the following commands from the munin/deploy directory:

  docker-composer -f reception.yml up

  docker-compose -f verifier.yml up

. link:images/04_drag_and_drop.gif[Drag and drop] a test file e.g.
link:https://github.com/xtuml/tower/blob/main/deploy/processed/FileRequest_HappyPath.json[`FileRequest_HappyPath.json`]
to the `/reception-incoming/` directory in your current directory. You will see
a flurry of output from the application and the file will reappear in the
`verifier-processed/` directory. Inspect the logs and you will see that the audit events
have been received, ordered, and verified by the application.

. Kill the process by pressing `Ctrl-C`.

=== Running the protocol verifier with the audit event simulator

In the last section, we built and ran the protocol verifier interactively with
test files. The audit event simulator can also be used to generate test audit
event files to be consumed by the AE Reception and AEO_SVDC processes. This 
section will also walk through using the process inspector to attach to the 
running processes and browse the instance population.

. Open an additional shell in the `munin/deploy` directory, run the following command
to launch the process with the simulator:

  docker compose -f ae-simulator.yml up

+
You will see a lot of output at once. If you look closely, you can see several
logs showing the simulator emitting audit events, followed by the AE Reception
and AEO_SVDC handling those audit events.

Clean up the process by running the following command:

  docker compose -f ae-simulator.yml down

=== Building a domain with test

Each domain contains a set of unit tests. We will use the `AEReception` domain
to demonstrate building a domain in test mode.

. Open a shell and navigate to the `AEReception` project directory:

  cd ~/git/munin/models/AEReception   # linux/mac
  cd C:\git\munin\models\AEReception  # windows

. Build the project in test mode with Docker by running the following command:

  docker compose -f docker-compose.test.yml build --no-cache

. Run the test schedule with Docker using the following command:

  docker compose -f docker-compose.test.yml up

A graphical rendition of the domain build process for test mode is depicted
link:images/Building_a_Domain_in_Test_Mode.pdf[here].
+
You will see a lot of output including some logs from the test runner. All of
the tests will be marked as "SUCCEEDED".

. Clean up the docker run with the following command:

  docker compose -f docker-compose.test.yml down
